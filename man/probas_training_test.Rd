% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/probas_training_test.R
\name{probas_training_test}
\alias{probas_training_test}
\title{Computing the relatives errors when predicting the membership probabilities on a validation dataset, for different values of ncoeff and npc}
\usage{
probas_training_test(
  outputs_train,
  outputs_test,
  density_ratio,
  prototypes,
  distance_func = function(A1, A2) {
     return(sqrt(sum((A1 - A2)^2)))
 },
  model_tuning = NULL,
  ncoeff_vec,
  npc_vec,
  return_pred = FALSE,
  outputs_pred = NULL,
  only_positive = FALSE,
  design_train,
  design_test,
  kernel = "matern5_2",
  wf = "d4",
  boundary = "periodic",
  J = 1,
  regmodel = "constant",
  normalize = FALSE,
  optim = "BFGS",
  objective = "LL",
  parameters = NULL,
  noise = FALSE,
  nugget = FALSE,
  seed = NULL,
  bias = rep(0, length(prototypes))
)
}
\arguments{
\item{outputs_train}{The training output samples on which the metamodel will be trained}

\item{outputs_test}{The validation output samples on which the metamodel performance will be evaluated}

\item{density_ratio}{density_ratio indicates the weight fX/g of each output}

\item{prototypes}{A set of l prototypes defining the Vorono√Ø cells}

\item{distance_func}{A function computing a distance between two elements in the output spaces.}

\item{model_tuning}{An optional list of models created for each ncoeff values.}

\item{ncoeff_vec}{A vector providing the different values of ncoeff to be tested. ncoeff fixes the number of coefficients used for PCA.}

\item{npc_vec}{A vector providing the different numbers of principal components to be tested.}

\item{return_pred}{A boolean indicating whether the predicted outputs should be returned or not}

\item{outputs_pred}{A list of the predicted outputs already obtained with the same parameters. Default is NULL.}

\item{only_positive}{A boolean indicating whether the predicted outputs should only contained positive values or not. Default is FALSE.}

\item{design_train}{a data frame representing the design of experiments of the training part.
The ith row contains the values of the d input variables corresponding
to the ith evaluation.}

\item{design_test}{a data frame representing the design of experiments of the validation part.
The ith row contains the values of the d input variables corresponding
to the ith evaluation.}

\item{kernel}{Character defining the covariance model: "exp", "gauss", "matern3_2", "matern5_2".}

\item{wf}{name of the wavelet filter to use in the decomposition}

\item{boundary}{a character string which specifies how boundaries are treated. Only "periodic" is currently implemented .}

\item{J}{depth of the wavelet decomposition, must be a number less than or equal to log(min(M,N),2). Default is 1.}

\item{regmodel}{Universal Kriging linear trend: "constant", "linear", "interactive".}

\item{normalize}{Logical. If TRUE both the input matrix X and the response y in normalized to take values in the interval [0, 1].}

\item{optim}{Character giving the Optimization method used to fit hyper-parameters. Possible values are: "BFGS", "Newton" and "none", the later simply keeping the values given in parameters. The method "BFGS" uses the gradient of the objective. The method "Newton" uses both the gradient and the Hessian of the objective.}

\item{objective}{Character giving the objective function to optimize. Possible values are: "LL" for the Log-Likelihood, "LOO" for the Leave-One-Out sum of squares and "LMP" for the Log-Marginal Posterior.}

\item{parameters}{Initial values for the hyper-parameters. When provided this must be named list with elements "sigma2" and "theta" containing the initial value(s) for the variance and for the range parameters. If theta is a matrix with more than one row, each row is used as a starting point for optimization.}

\item{noise}{Boolean specifying whether to execute NoiseKriging or not}

\item{nugget}{Boolean specifying whether to execute NuggetKriging or not}

\item{seed}{An optional random seed}

\item{bias}{A vector indicating the bias that came out when computing the importance sampling estimators of the membership probabilities. Each element of the vector is associated to a Voronoi cell. Default is 0 for all Voronoi cells.}
}
\value{
A list containing several outputs :
- probas_pred_df a dataframe indicating for each pair (npc, ncoeff) the obtained predicted membership probabilities
- relative_error_df a dataframe indicating for each pair (npc, ncoeff) the relative error when predicting the membership probabilities
- outputs_pred an array providing the predicted outputs if return_pred is TRUE. If return_pred is FALSE, then outputs_pred is NULL.
}
\description{
Computing the relatives errors when predicting the membership probabilities on a validation dataset, for different values of ncoeff and npc
}
\examples{
func2D <- function(X){
Zgrid <- expand.grid(z1 = seq(-5,5,l=20),z2 = seq(-5,5,l=20))
n<-nrow(X)
Y <- lapply(1:n, function(i){X[i,]*exp(-((0.8*Zgrid$z1+0.2*Zgrid$z2
-10*X[i,])**2)/(60*X[i,]**2))*(Zgrid$z1-Zgrid$z2)*cos(X[i,]*4)})
Ymaps<- array(unlist(Y),dim=c(20,20,n))
return(Ymaps)
}
design_train = data.frame(X = seq(-1,1,l= 8))
outputs_train = func2D(design_train)
design_test = data.frame(X = seq(-0.99,0.99,l=50))
outputs_test = func2D(design_test)
prototypes = lapply(c(10,20,30,40,50), function(i){outputs_test[,,i]})
density_ratio = rep(1, 50)
distance_func = function(A1,A2){return(sqrt(sum((A1-A2)^2)))}
list_probas_train_test = probas_training_test(prototypes = prototypes,
density_ratio = density_ratio, distance_func = distance_func, return_pred = TRUE,
 outputs_train = outputs_train, outputs_test = outputs_test,
 ncoeff_vec = c(50,100,200,400), npc_vec = 2:4, design_train = design_train,
design_test = design_test)
}
